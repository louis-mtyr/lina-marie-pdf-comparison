{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f62cf5",
   "metadata": {},
   "source": [
    "# Pré-requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-generativeai pypdf sentence-transformers faiss-cpu numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80db8a",
   "metadata": {},
   "source": [
    "# Extraction PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee6bed",
   "metadata": {},
   "source": [
    "### pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd8815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Obtaining dependency information for pdf2image from https://files.pythonhosted.org/packages/62/33/61766ae033518957f877ab246f87ca30a85b778ebaad65b7f74fa7e52988/pdf2image-1.17.0-py3-none-any.whl.metadata\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pytesseract\n",
      "  Obtaining dependency information for pytesseract from https://files.pythonhosted.org/packages/7a/33/8312d7ce74670c9d39a532b2c246a853861120486be9443eebf048043637/pytesseract-0.3.13-py3-none-any.whl.metadata\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\louis\\anaconda3\\lib\\site-packages (from pdf2image) (9.4.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from pytesseract) (23.1)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract, pdf2image\n",
      "Successfully installed pdf2image-1.17.0 pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pdf2image pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309ccd0",
   "metadata": {},
   "source": [
    "### travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c424ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2a2b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gynécologie\n",
      "Obstétrique\n",
      "Front matterChez le même éditeur\n",
      "Dans la même collection\n",
      "Activité physique et sportive : facteur de santé, par le Collège français des enseignants en médecine et trauma-\n",
      "tologie du sport et de l'exercice physique (CFEMTSEP), 2019, 96 pages.\n",
      "Anatomie et cytologie pathologiques, par le Collège français des pathologistes (CoPath), 3 e édition, 2019, \n",
      "416 pages.\n",
      "Chirurgie maxillo-faciale et stomatologie, par le Collège hospitalo-universitaire français de chirurgie maxillofa -\n",
      "ciale et stomatologie, 5e édition, 2021, 432 pages.\n",
      "Dermatologie, par le Collège des enseignants en dermatologie de France (CEDEF), 7e édition, 2017, 472 pages.\n",
      "Endocrinologie, diabétologie et maladies métaboliques, par le Collège des enseignants d'endocrinologie, dia -\n",
      "bète et maladies métaboliques (CEEDMM), 5e édition, 2021, 568 pages.\n",
      "Gériatrie, par le Collège national des enseignants de gériatrie (CNEG), 5e édition, 2021, 400 pages.\n",
      "Gynécologie obstétrique, par le Collège national des gynéc\n",
      "#################\n",
      "Gynécologie \n",
      "ObstétriqueGynécologie \n",
      "Obstétrique\n",
      "Sous l’égide du Collège National des Gynécologues et Obstétriciens Français \n",
      "et du Collège des Enseignants de Gynécologie-Obstétrique\n",
      "Coordonné par :\n",
      "Philippe Deruelle et Raffaèle Fauvet\n",
      "6\n",
      "e\n",
      " éditionElsevier Masson SAS, 65, rue Camille-Desmoulins, 92442 Issy-les-Moulineaux cedex \n",
      "www.elsevier-masson.fr\n",
      "Gynécologie – Obstétrique , 6\n",
      "e\n",
      " édition, par Philippe Deruelle, Raffaèle Fauvet, le Collège National des Gynécologues \n",
      "et Obstétriciens Français et le Collège des Enseignants de Gynécologie-Obstétrique.\n",
      "© 2024 Elsevier Masson SAS \n",
      "ISBN : 978 - 2-294 - 78091 - 2\n",
      "e-ISBN : 978 - 2-294 - 78214 - 5\n",
      "Tous droits réservés, y compris ceux relatifs à la fouille de textes et de données, à l’entraînement de l’intelligence \n",
      "artificielle et aux technologies similaires.\n",
      "Note de l’éditeur : Elsevier adopte une position neutre en ce qui concerne les conflits territoriaux ou les revendi-\n",
      "cations juridictionnelles dans les contenus qu’il publie, y compris d\n"
     ]
    }
   ],
   "source": [
    "old_doc = extract_text_from_pdf(\"ancien_clean.pdf\")\n",
    "new_doc = extract_text_from_pdf(\"nouveau.pdf\")\n",
    "\n",
    "print(old_doc[:1000])\n",
    "print(\"#################\")\n",
    "print(new_doc[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253611c",
   "metadata": {},
   "source": [
    "### saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ca3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saves/ancien.txt\", \"w\") as f:\n",
    "    f.write(old_doc)\n",
    "with open(\"saves/nouveau.txt\", \"w\") as f:\n",
    "    f.write(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2163f6",
   "metadata": {},
   "source": [
    "# Splitting en chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df08c92",
   "metadata": {},
   "source": [
    "### pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ca2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\louis\\anaconda3\\lib\\site-packages (0.3.25)\n",
      "Collecting tiktoken\n",
      "  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/6f/07/c67ad1724b8e14e2b4c8cca04b15da158733ac60136879131db05dda7c30/tiktoken-0.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\louis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\louis\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\louis\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.0)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "   ---------------------------------------- 0.0/893.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/893.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 71.7/893.9 kB 975.2 kB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 399.4/893.9 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 893.9/893.9 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13d390",
   "metadata": {},
   "source": [
    "### travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc81db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    chunks = []\n",
    "    # Une approche simple par caractères, mais vous pouvez améliorer avec un split par paragraphe ou phrase\n",
    "    words = text.split()\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) + 1 <= chunk_size:\n",
    "            current_chunk.append(word)\n",
    "            current_length += len(word) + 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap:] + [word] # Garder un chevauchement\n",
    "            current_length = sum(len(w) + 1 for w in current_chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def split_text_langchain(text: str, chunk_size: int = 250, overlap: int = 25) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "766cc093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de chunks dans l'ancien document: 3295\n",
      "Premier chunk de l'ancien document: Gynécologie\n",
      "Obstétrique\n",
      "Front matterChez le même éditeur\n",
      "Dans la même collection\n",
      "Activité physique et sportive : facteur de santé, par le Collège français des enseignants en médecine et trauma-\n",
      "tologie du sport et de l'exercice physique (CFEMTSEP), 2019, 96 pages.\n",
      "Anatomie et cytologie pathologiques, par le Collège français des pathologistes (CoPath), 3 e édition, 2019, \n",
      "416 pages.\n",
      "Chirurgie maxillo-faciale et stomatologie, par le Collège hospitalo-universitaire français de chirurgie maxillofa -\n",
      "ciale et stomatologie, 5e édition, 2021, 432 pages.\n",
      "Dermatologie, par le Collège des enseignants en dermatologie de France (CEDEF), 7e édition, 2017, 472 pages.\n",
      "Nombre de chunks dans le nouveau document: 3279\n",
      "Premier chunk du nouveau document: Gynécologie \n",
      "ObstétriqueGynécologie \n",
      "Obstétrique\n",
      "Sous l’égide du Collège National des Gynécologues et Obstétriciens Français \n",
      "et du Collège des Enseignants de Gynécologie-Obstétrique\n",
      "Coordonné par :\n",
      "Philippe Deruelle et Raffaèle Fauvet\n",
      "6\n",
      "e\n",
      " éditionElsevier Masson SAS, 65, rue Camille-Desmoulins, 92442 Issy-les-Moulineaux cedex \n",
      "www.elsevier-masson.fr\n",
      "Gynécologie – Obstétrique , 6\n",
      "e\n",
      " édition, par Philippe Deruelle, Raffaèle Fauvet, le Collège National des Gynécologues \n",
      "et Obstétriciens Français et le Collège des Enseignants de Gynécologie-Obstétrique.\n",
      "© 2024 Elsevier Masson SAS\n"
     ]
    }
   ],
   "source": [
    "chunks_old = split_text_langchain(old_doc)\n",
    "chunks_new = split_text_langchain(new_doc)\n",
    "\n",
    "print(f\"Nombre de chunks dans l'ancien document: {len(chunks_old)}\")\n",
    "print(f\"Premier chunk de l'ancien document: {chunks_old[0]}\")\n",
    "print(f\"Nombre de chunks dans le nouveau document: {len(chunks_new)}\")\n",
    "print(f\"Premier chunk du nouveau document: {chunks_new[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f4aa6",
   "metadata": {},
   "source": [
    "### saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1516e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"saves/chunks/ancien_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_old, f)\n",
    "with open(\"saves/chunks/nouveau_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks_new, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39015b06",
   "metadata": {},
   "source": [
    "# Encoding & Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ceeaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Charger un modèle d'embedding\n",
    "# 'all-MiniLM-L6-v2' est un bon compromis taille/performance pour le texte général.\n",
    "# Pour des cas plus spécifiques, vous pourriez envisager des modèles plus grands.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_embeddings(texts: List[str]):\n",
    "    return model.encode(texts, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# embeddings1 = get_embeddings(chunks1)\n",
    "# embeddings2 = get_embeddings(chunks2)\n",
    "\n",
    "# Créer des index FAISS pour chaque document\n",
    "# dimension_embedding = embeddings1.shape[1]\n",
    "# index1 = faiss.IndexFlatL2(dimension_embedding)\n",
    "# index1.add(embeddings1)\n",
    "\n",
    "# index2 = faiss.IndexFlatL2(dimension_embedding)\n",
    "# index2.add(embeddings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92fd5e",
   "metadata": {},
   "source": [
    "# Comparaison & identification des différences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3df89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_differences(chunks1: List[str], embeddings1: np.ndarray, index1: faiss.IndexFlatL2,\n",
    "                     chunks2: List[str], embeddings2: np.ndarray, index2: faiss.IndexFlatL2,\n",
    "                     similarity_threshold: float = 0.8) -> List[str]:\n",
    "    differences = []\n",
    "\n",
    "    # Parcourir les chunks du document 2 et chercher leur correspondance dans le document 1\n",
    "    for i, chunk2 in enumerate(chunks2):\n",
    "        query_embedding = embeddings2[i].reshape(1, -1)\n",
    "        distances, indices = index1.search(query_embedding, k=1) # Chercher le plus proche dans doc1\n",
    "\n",
    "        closest_distance = distances[0][0]\n",
    "        closest_chunk1_index = indices[0][0]\n",
    "\n",
    "        # Si la distance est grande (faible similarité), ou si le chunk n'est pas \"trouvé\" (distance > seuil),\n",
    "        # cela indique une possible différence ou un ajout.\n",
    "        # Note: L2 distance plus faible = plus similaire.\n",
    "        # Vous devrez ajuster le seuil en fonction de vos observations.\n",
    "        if closest_distance > (1 - similarity_threshold): # Adapter la condition pour L2 distance\n",
    "            differences.append(f\"Ajout/Modification probable dans Doc2: '{chunk2}' (Distance au plus proche de Doc1: {closest_distance:.4f})\")\n",
    "        else:\n",
    "            # Pour des modifications subtiles, vous pouvez aussi comparer le chunk original avec le chunk trouvé\n",
    "            # si la similarité est juste au-dessus du seuil mais pas parfaite.\n",
    "            # Cela nécessiterait une étape supplémentaire d'analyse.\n",
    "            pass # Ici, le chunk est considéré comme similaire\n",
    "\n",
    "    # Vous pouvez aussi faire l'inverse (chercher les différences de doc1 par rapport à doc2)\n",
    "    # selon votre besoin de détection des suppressions.\n",
    "\n",
    "    return differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89753e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def detailed_chunk_diff(chunk1: str, chunk2: str) -> List[str]:\n",
    "    d = difflib.Differ()\n",
    "    diff = list(d.compare(chunk1.splitlines(), chunk2.splitlines()))\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb47a14",
   "metadata": {},
   "source": [
    "# Agent synthétisation différences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configurez votre clé API Gemini\n",
    "# genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) # Ou directement genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def summarize_differences_with_gemini(differences: List[str]):\n",
    "    if not differences:\n",
    "        return \"Aucune différence significative détectée entre les documents.\"\n",
    "\n",
    "    # Joindre les différences pour les présenter au modèle\n",
    "    differences_text = \"\\n\".join(differences)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Voici une liste de différences potentielles détectées entre deux documents.\n",
    "    Veuillez analyser ces différences et fournir une liste claire et concise de chaque ajout ou modification significative,\n",
    "    en expliquant brièvement de quoi il s'agit.\n",
    "\n",
    "    Différences détectées :\n",
    "    {differences_text}\n",
    "\n",
    "    Liste des ajouts/modifications:\n",
    "    \"\"\"\n",
    "\n",
    "    # Utilisez Gemini 1.5 Flash\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "    # Ajustez max_output_tokens si vos résumés sont très longs\n",
    "    response = model.generate_content(prompt, generation_config={\"max_output_tokens\": 800})\n",
    "\n",
    "    try:\n",
    "        return response.text\n",
    "    except ValueError as e:\n",
    "        print(f\"Erreur lors de la génération de la réponse : {e}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Candidats de réponse: {response.candidates}\")\n",
    "        return \"Impossible de générer le résumé des différences.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0271c2",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59da9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettez vos chemins de fichiers PDF ici\n",
    "PDF_DOC1 = \"document1.pdf\"\n",
    "PDF_DOC2 = \"document2.pdf\"\n",
    "\n",
    "# 1. Extraction du texte\n",
    "doc1_text = extract_text_from_pdf(PDF_DOC1)\n",
    "doc2_text = extract_text_from_pdf(PDF_DOC2)\n",
    "\n",
    "# 2. Division en chunks\n",
    "chunks1 = split_text_into_chunks(doc1_text, chunk_size=300, overlap=30) # Ajustez les tailles de chunk\n",
    "chunks2 = split_text_into_chunks(doc2_text, chunk_size=300, overlap=30)\n",
    "\n",
    "print(f\"Document 1 a {len(chunks1)} chunks.\")\n",
    "print(f\"Document 2 a {len(chunks2)} chunks.\")\n",
    "\n",
    "# 3. Embedding et vectorisation\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings1 = get_embeddings(chunks1)\n",
    "embeddings2 = get_embeddings(chunks2)\n",
    "\n",
    "dimension_embedding = embeddings1.shape[1]\n",
    "index1 = faiss.IndexFlatL2(dimension_embedding)\n",
    "index1.add(embeddings1)\n",
    "index2 = faiss.IndexFlatL2(dimension_embedding)\n",
    "index2.add(embeddings2)\n",
    "\n",
    "\n",
    "# 4. Identification des différences\n",
    "# Seuils: la similarité cosinus va de -1 à 1. Une valeur de 0.8 signifie 80% de similarité.\n",
    "# Pour la distance L2, une valeur plus petite signifie plus similaire. 0 étant identique.\n",
    "# Si vous avez converti votre similarité cosinus en distance L2 (distance = 1 - similarité),\n",
    "# alors un seuil de 0.2 (1-0.8) serait approprié.\n",
    "# Vous devrez expérimenter avec ce seuil.\n",
    "potential_differences = find_differences(chunks1, embeddings1, index1,\n",
    "                                         chunks2, embeddings2, index2,\n",
    "                                         similarity_threshold=0.85)\n",
    "\n",
    "print(f\"Nombre de différences potentielles détectées : {len(potential_differences)}\")\n",
    "# print(\"\\n\".join(potential_differences[:5])) # Affiche les 5 premières pour un aperçu\n",
    "\n",
    "# 5. Synthèse avec Gemini\n",
    "# Assurez-vous que votre clé API est configurée\n",
    "# import os\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"VOTRE_CLE_API\" # REMPLACEZ PAR VOTRE VRAIE CLE API\n",
    "# genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "if potential_differences:\n",
    "    final_summary = summarize_differences_with_gemini(potential_differences)\n",
    "    print(\"\\nRésumé des différences par Gemini :\")\n",
    "    print(final_summary)\n",
    "else:\n",
    "    print(\"Aucune différence significative trouvée nécessitant une synthèse par Gemini.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
